{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claredavies/ImageIndexing/blob/master/Image_Indexing_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a795ccf4"
      },
      "source": [
        "# 11762 Content-Based Image Retrieval\n",
        "## Master's Degree in Intelligent Systems\n",
        "### University of the Balearic Islands\n",
        "\n",
        "---\n",
        "\n",
        "**Before you turn this problem in, please put your full names and DNIs (or NIEs) below, and execute the cell:**"
      ],
      "id": "a795ccf4"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6947b975"
      },
      "outputs": [],
      "source": [
        "NAME  = \"Clare Davies\"\n",
        "DNI   = \"99999999R\""
      ],
      "id": "6947b975"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OBYyYOwNu7hY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b450676-7d22-42bf-9dd6-a5e59fc8365a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "OBYyYOwNu7hY"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cuu5vW4ivs1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81645014-464b-4ded-f777-35b8f07d3637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ImageIndexing\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/ImageIndexing"
      ],
      "id": "cuu5vW4ivs1C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec32c2c0"
      },
      "source": [
        "Make sure you fill in any place that says `YOUR CODE HERE` or `YOUR ANSWER HERE`. **Justify** all of your answers, **graphically** wherever possible. Remember that this notebook will be considered as a report to the work done during the assignment.\n",
        "\n",
        "---"
      ],
      "id": "ec32c2c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BTyYK7ivdfy"
      },
      "outputs": [],
      "source": [
        "!unzip iric_dev_kit.zip"
      ],
      "id": "6BTyYK7ivdfy"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a32b507a"
      },
      "outputs": [],
      "source": [
        "# Setup code for this assignment\n",
        "import cv2\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.cluster.vq as vq\n",
        "import tqdm\n",
        "import zipfile\n",
        "\n",
        "## Adding parent folder to find other libs\n",
        "import sys\n",
        "if \"..\" not in sys.path:\n",
        "    sys.path.insert(0,\"..\")\n",
        "    \n",
        "import iric_dev_kit.iric_utils.eval_holidays as ev\n",
        "import iric_dev_kit.iric_utils.read_descriptors as rd\n",
        "\n",
        "# Configuring Matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "a32b507a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d621778b"
      },
      "source": [
        "## Introduction\n",
        "In this assignment, you will implement and evaluate different methods for indexing images. As usual during this course, we will use the [INRIA Holidays](http://lear.inrialpes.fr/people/jegou/data.php) dataset. **Check the Assignment 1 to further information about this dataset.**\n",
        "\n",
        "We also need the provided script to evaluate a CBIR system on this dataset. Remember that the performance is measured computing the **mean average precision** (mAP) over all queries. **Check also the Assignment 1 to remember how to use this script and the different functions it offers.**"
      ],
      "id": "d621778b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77b3c399"
      },
      "source": [
        "### Loading images\n",
        "As we did in Assignment 1, for managing images, we will create four lists:\n",
        "- **`query_names`**: File names of the *query* images\n",
        "- **`query_imgs`**: *Query* images loaded using OpenCV2\n",
        "- **`train_names`**: File names of the *train* (database) images\n",
        "- **`train_imgs`**: *Train* images loaded using OpenCV2\n",
        "\n",
        "In this assignment, we will use the original holidays dataset:"
      ],
      "id": "77b3c399"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWfI1BLVwmqV"
      },
      "outputs": [],
      "source": [
        "!unzip holidays_mini.zip"
      ],
      "id": "IWfI1BLVwmqV"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78adf5f7",
        "outputId": "c2e7a138-5d8e-4cef-8e09-6da2ce52c43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n",
            "31\n"
          ]
        }
      ],
      "source": [
        "# Separating the dataset into query and train images\n",
        "query_names = []\n",
        "query_imgs = []\n",
        "train_names = []\n",
        "train_imgs = []\n",
        "\n",
        "# with open('../holidays/holidays_images.dat') as f:\n",
        "\n",
        "with open('holidays_mini/holidays_images.dat') as f:\n",
        "    for line in f:\n",
        "        imname = line.strip()\n",
        "        imno = int(imname[:-len(\".jpg\")])\n",
        "        # img = cv2.imread('../holidays/images/' + imname)\n",
        "\n",
        "        img = cv2.imread('holidays_mini/images/' + imname)\n",
        "        # Resize the images for a faster operation in this assignment\n",
        "        img = cv2.resize(img, None, fx=0.25, fy=0.25, interpolation = cv2.INTER_CUBIC)\n",
        "    \n",
        "        # Checking if this is a query image\n",
        "        if imno % 100 == 0:\n",
        "            query_names.append(imname)\n",
        "            query_imgs.append(img)\n",
        "        else:\n",
        "            train_names.append(imname)\n",
        "            train_imgs.append(img)\n",
        "\n",
        "print(len(query_names))\n",
        "print(len(train_names))"
      ],
      "id": "78adf5f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27d7080b"
      },
      "source": [
        "## Loading SIFT descriptors\n",
        "In this assignment we will create four additional lists:\n",
        "- **`query_kps`**: A list of lists of keypoints (cv2.KeyPoint) extracted from the *query* images\n",
        "- **`query_desc`**: A list of numpy arrays including, for each set of keypoints, the SIFT descriptors extracted from the *query* images\n",
        "- **`train_kps`**: A list of lists of keypoints (cv2.KeyPoint) extracted from the *train* (database) images\n",
        "- **`train_desc`**: A list of numpy arrays including, for each set of keypoints, the SIFT descriptors extracted from the *train* images\n",
        "\n",
        "Unlike in Assigment 1, now you will be provided with a set of SIFT descriptors for each image, and, therefore, you do not need to create these lists from scratch. First, download the descriptors from [here](https://uibes-my.sharepoint.com/:u:/g/personal/egf350_id_uib_es/Eam8Ld8YDaJAhNr91YVdAZIB_wVZJ8kzzKD7BR6R3LziMw).\n",
        "\n",
        "> **Unzip this file into the root directory of the development kit, at the same level of the datasets.**\n",
        "\n",
        "Now, a new directory called `siftgeo` should be in your workspace, containing the set of SIFT descriptors for each image of the dataset. These descriptors are stored in binary format and, thus, you are also provided with some tools to load them. To be more precise, you can call the function `load_SIFT_descriptors` to load the descriptors of a list of images:"
      ],
      "id": "27d7080b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B822YyX-UVsM"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "def unzip_tarfile(filename):\n",
        "  try:\n",
        "    tar = tarfile.open(filename)\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "  except EOFError:\n",
        "    print(\"error\")"
      ],
      "id": "B822YyX-UVsM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQp4svdOw2Hs"
      },
      "outputs": [],
      "source": [
        "unzip_tarfile(\"siftgeo.tar.gz\")"
      ],
      "id": "sQp4svdOw2Hs"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1BBTN1FRxbhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b00f61-5b89-4bd7-ab6a-78fd6dff2bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ImageIndexing/iric_dev_kit\n"
          ]
        }
      ],
      "source": [
        "%cd iric_dev_kit"
      ],
      "id": "1BBTN1FRxbhz"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f7e7e1c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae898e31-2964-411e-bf92-be785507a63d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n",
            "31\n",
            "19\n",
            "31\n",
            "(1000, 128)\n",
            "[[10.  6. 52. ... 15.  4.  0.]\n",
            " [16. 50. 12. ... 15.  4.  0.]\n",
            " [10. 11. 58. ...  7.  4.  4.]\n",
            " ...\n",
            " [27. 15.  0. ... 16.  8. 12.]\n",
            " [51. 47. 14. ... 35. 26.  0.]\n",
            " [ 2. 37. 25. ... 47. 13.  8.]]\n"
          ]
        }
      ],
      "source": [
        "# Loading descriptors\n",
        "query_kps, query_desc = rd.load_SIFT_descriptors(query_names, max_desc=1000)\n",
        "train_kps, train_desc = rd.load_SIFT_descriptors(train_names, max_desc=1000)\n",
        "\n",
        "# Some prints\n",
        "print(len(query_kps))\n",
        "print(len(train_kps))\n",
        "print(len(query_desc))\n",
        "print(len(train_desc))\n",
        "print(query_desc[0].shape)\n",
        "print(query_desc[0])"
      ],
      "id": "f7e7e1c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b8f8d0c"
      },
      "source": [
        "For development purposes, we use the parameter `max_desc` to load a maximum number (1000) of the descriptors. This will speed up the execution of the rest of the notebook, while the decrease in performance will be minimum.\n",
        "\n",
        "> **Some images do not have keypoints/descriptors. Take this into account when you develop your solution.**"
      ],
      "id": "6b8f8d0c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2aeee4"
      },
      "source": [
        "## $k$-d trees and LSH \n",
        "Let's start coding. At this section, you will develop a retrieval system using $k$-d trees and Locality Sensitive Hashing (LSH). "
      ],
      "id": "9e2aeee4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b5ec641"
      },
      "source": [
        "### General framework\n",
        "As we did in the first assignment, you first will develop some utilities to simplify your work. Write a function called `search_image` to search an image in a generic index (database). You should search each descriptor of the given query image and obtain its two closest SIFT descriptors in the database. Next, the initial set of matches should be filtered using the **NNDR criterion (use 0.8 as ratio)**, as you did in the previous assignment. For each database image, its final score with regard to this query image will be the **number of correct matches** with this image:"
      ],
      "id": "9b5ec641"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHzWmAUYU7Na"
      },
      "source": [
        "The 2 matches returned have 0 distance!\n",
        "\n"
      ],
      "id": "pHzWmAUYU7Na"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d9560c34"
      },
      "outputs": [],
      "source": [
        "def search_image(descs, index, id_to_name):\n",
        "    \"\"\"\n",
        "    Search an image in the index\n",
        "    \n",
        "    - descs: A numpy array. This is the set descriptors extracted from the query image\n",
        "    - index: OpenCV FLANN index to search for descriptors.\n",
        "    - id_to_name: An associative list to link every image index to its real name\n",
        "        e.g. id_to_name[0] = '100001.jpg', id_to_name[1] = '100002.jpg'\n",
        "  \n",
        "    RETURN: \n",
        "    - An ordered list of similar images, e.g.: ['100101.jpg', '100202.jpg', ...]\n",
        "  \n",
        "    \"\"\"\n",
        "    # from google.colab.patches import cv2_imshow\n",
        "    # img = cv2.imread('holidays_mini/images/' + query_names[0])\n",
        "    # cv2_imshow(img)\n",
        "\n",
        "    for desc in descs:\n",
        "      # Perform FLANN-based matching to find similar descriptors\n",
        "      matches = index.knnMatch(queryDescriptors=descs, k=2)\n",
        "      # print(\"queryidx: \", matches[0][0].queryIdx)\n",
        "      # print(\"trainIdx: \", matches[0][0].trainIdx)\n",
        "      # print(\"imgIdx: \", matches[0][0].imgIdx)\n",
        "      # print(\"distance: \", matches[0][0].distance)\n",
        "\n",
        "    # Apply ratio test to filter out bad matches\n",
        "    good_matches = []\n",
        "    for m, n in matches:\n",
        "        if m.distance < 0.8 * n.distance:\n",
        "            good_matches.append(m)\n",
        "    \n",
        "    # Collect indices of similar images\n",
        "    similar_ids = set()\n",
        "    for m in good_matches:\n",
        "        img_idx = m.imgIdx\n",
        "        similar_ids.add(img_idx)\n",
        "    \n",
        "    # Convert indices to image names\n",
        "    similar_names = []\n",
        "    for id in similar_ids:\n",
        "        similar_names.append(id_to_name[id])\n",
        "    \n",
        "    # Return ordered list of similar images\n",
        "    return sorted(similar_names)"
      ],
      "id": "d9560c34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfdqDu4mYB-H",
        "outputId": "39a09a3e-7589-4c4b-9fe3-990277ad73db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['100001.jpg', '100002.jpg']\n"
          ]
        }
      ],
      "source": [
        "print(train_names[0:2])"
      ],
      "id": "CfdqDu4mYB-H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gWME8MdYbRY",
        "outputId": "e20e0c83-476a-49ca-a9fc-96321f326541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100000.jpg\n"
          ]
        }
      ],
      "source": [
        "print(query_names[0])"
      ],
      "id": "4gWME8MdYbRY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5470e6a8"
      },
      "source": [
        "Now, write a function called `compute_mAP`. Given a list of query images and a trained index, this function should return a Python dictionary with the ordered results for each query along with the computed mAP:"
      ],
      "id": "5470e6a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0ZbKLFRAbiI"
      },
      "outputs": [],
      "source": [
        "print(query_desc[0])"
      ],
      "id": "u0ZbKLFRAbiI"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0195c2f4"
      },
      "outputs": [],
      "source": [
        "def compute_mAP(query_names, query_desc, index, id_to_name):\n",
        "    \"\"\"\n",
        "    Perform a search for a list of query images against the database.\n",
        "    \n",
        "    - query_names: An ordered list with the names of the query images\n",
        "    - query_desc: A list containing numpy arrays of size (ndesc_for_this_image, 128)\n",
        "                  Each numpy array i corresponds to the descriptors found at image i\n",
        "    - index: FLANN index\n",
        "    - id_to_name: An associative array to link every image index to its real name\n",
        "                  e.g. id_to_name[0] = '100001.jpg', id_to_name[1] = '100002.jpg'\n",
        "  \n",
        "    RETURN: \n",
        "    - total_results: A dictionary containing, for each query image, an sorted list of the database images\n",
        "    - m_ap: Mean Average Precision averaged over all queries\n",
        "    \"\"\"\n",
        "    total_results = {}\n",
        "    aps = []\n",
        "    \n",
        "    # Build a set of randomized k-d trees\n",
        "    index_params = dict(algorithm=1, trees=4)\n",
        "    search_params = dict(checks=100)\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "    # Perform a search for each query image against the database\n",
        "    for qname, qdesc in zip(query_names, query_desc):\n",
        "        # Search for similar images\n",
        "        results = search_image(qdesc, index, id_to_name)\n",
        "        \n",
        "        # Calculate average precision\n",
        "        tp = 0\n",
        "        fp = 0\n",
        "        ap = 0\n",
        "        for i, res in enumerate(results):\n",
        "            if res == qname:\n",
        "                tp += 1\n",
        "                ap += tp / (i + 1)\n",
        "            else:\n",
        "                fp += 1\n",
        "        if tp > 0:\n",
        "            ap /= tp\n",
        "            aps.append(ap)\n",
        "        else:\n",
        "            aps.append(0)\n",
        "        \n",
        "        # Store results for this query image\n",
        "        total_results[qname] = results\n",
        "    \n",
        "    # Calculate mean average precision\n",
        "    m_ap = np.mean(aps)\n",
        "    \n",
        "    return total_results, m_ap"
      ],
      "id": "0195c2f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7782f8"
      },
      "source": [
        "### $k$-d Trees\n",
        "In this section you will use a set of randomized $k$-d trees to index the database of images. Write a function called `build_db_kdtrees` to build a set of randomized $k$-d trees given a set of descriptors:\n",
        "\n",
        "> **Useful links**: [cv2.FlannBasedMatcher](https://docs.opencv.org/4.5.5/dc/de2/classcv_1_1FlannBasedMatcher.html), [Possible algorithms to create an index](https://docs.opencv.org/4.5.5/db/d18/classcv_1_1flann_1_1GenericIndex.html#a8fff14185f9f3d2f2311b528f65b146c), [Algorithms IDs](https://github.com/opencv/opencv/blob/master/modules/flann/include/opencv2/flann/defines.h#L70)"
      ],
      "id": "6a7782f8"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3586111e"
      },
      "outputs": [],
      "source": [
        "def build_db_kdtrees(descs, ntrees = 4):\n",
        "    \"\"\"\n",
        "    Build a set of randomized k-d trees.\n",
        "    \n",
        "    - descs: A list of length len(img_names) where each element is a numpy array \n",
        "        of size (ndesc_for_this_image, 128). Each numpy array i corresponds \n",
        "        to the descriptors found on image i\n",
        "    - ntrees: Number of trees to train\n",
        "  \n",
        "    RETURN: \n",
        "    - index: Trained FLANN index\n",
        "    \"\"\"  \n",
        "  \n",
        "    FLANN_INDEX_KDTREE = 1\n",
        "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=ntrees)\n",
        "    search_params = dict(checks=32)\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    flann.add(descs)\n",
        "    flann.train()\n",
        "    return flann"
      ],
      "id": "3586111e"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fe39d9d",
        "outputId": "bb5393aa-b8a1-4244-e4a2-bee203cbb455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "# Simple example of DB construction\n",
        "index = build_db_kdtrees(train_desc[0:2])\n",
        "print(len(index.getTrainDescriptors()))"
      ],
      "id": "2fe39d9d"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8598581a",
        "outputId": "f8b882dd-beda-4105-b863-0090036b7b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['100001.jpg', '100002.jpg']\n"
          ]
        }
      ],
      "source": [
        "# Search an image in the index\n",
        "img_res = search_image(query_desc[0], index, train_names[0:2])\n",
        "print(img_res)"
      ],
      "id": "8598581a"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0c32c81c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2ac888-8576-4663-f57c-3252166a5d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['100001.jpg', '100002.jpg']\n",
            "['100001.jpg', '100002.jpg']\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "# Example of computing mAP\n",
        "results, mAP = compute_mAP(query_names, query_desc, index, train_names[0:2])\n",
        "print(results['100000.jpg'])\n",
        "print(results['100100.jpg'])\n",
        "print(mAP) # This should be 0 now, since there is only two images in the database."
      ],
      "id": "0c32c81c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec669537"
      },
      "source": [
        "**Q1**: Using functions developed so far, in the following cell compute the resulting **mAP** of the system **using 4 trees**:"
      ],
      "id": "ec669537"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3d0cfe3"
      },
      "outputs": [],
      "source": [
        "# Fill this variable with the resulting mAP\n",
        "mAP_kdtree = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "id": "d3d0cfe3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ef5289"
      },
      "outputs": [],
      "source": [
        "print('mAP: %.5f' % mAP_kdtree)"
      ],
      "id": "f3ef5289"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57bae631"
      },
      "source": [
        "**Q2**: Are the results stable? Do you obtain always the same mAP? Why?"
      ],
      "id": "57bae631"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72827768"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "72827768"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8baed384"
      },
      "source": [
        "**Q3:** Analyze the effect of changing the number of trees in terms of mAP and average response time. Some plots here can be useful to justify your answer."
      ],
      "id": "8baed384"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e57caf1"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "3e57caf1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52633ad1"
      },
      "source": [
        "### Locality Sensitive Hashing (LSH)\n",
        "In this section, you will use LSH to index the database of images. The LSH implementation included in OpenCV uses **bit sampling** for **Hamming distance** as a hash function and, therefore, binary descriptors must be used. Hence, SIFT descriptors are not valid and we need to describe the images, but using, for instance, ORB.\n",
        "\n",
        "In the following cell, write the code required to generate **roughly 1500 keypoints / descriptors** using ORB for each query / train image:\n",
        "\n",
        "> **Useful links**: [cv2.ORB_create](https://docs.opencv.org/4.5.4/db/d95/classcv_1_1ORB.html#aeff0cbe668659b7ca14bb85ff1c4073b)"
      ],
      "id": "52633ad1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b1223cb"
      },
      "outputs": [],
      "source": [
        "def generateAllDescriptorsAndKeypoints(query_images, train_images, no_keypoints=1500, no_descriptors=1500):\n",
        "    \"\"\"\n",
        "    In this section, you will use LSH to index the database of images. The LSH implementation \n",
        "    included in OpenCV uses bit sampling for Hamming distance as a hash function and, therefore, \n",
        "    binary descriptors must be used. Hence, SIFT descriptors are not valid and we need to describe the images\n",
        "    , but using, for instance, ORB.\n",
        "\n",
        "    Useful links: cv2.ORB_create\n",
        "\n",
        "    generate roughly 1500 keypoints / descriptors using ORB for each query / train image:\n",
        "    \n",
        "    - descs: A list of length len(img_names) where each element is a numpy array \n",
        "        of size (ndesc_for_this_image, 128). Each numpy array i corresponds \n",
        "        to the descriptors found on image i\n",
        "    - ntrees: Number of trees to train\n",
        "\n",
        "    RETURN: \n",
        "    - query_kps_orb: list of keypoints for query images\n",
        "    - query_desc_orb: list of descriptors for query images\n",
        "    - train_kps_orb: list of keypoints for train images\n",
        "    - train_desc_orb: list of descriptors for train images\n",
        "    \"\"\" \n",
        "    \n",
        "    orb = cv2.ORB_create(nfeatures=no_keypoints, nlevels=8, scoreType=cv2.ORB_FAST_SCORE)\n",
        "\n",
        "    # Initialize keypoint and descriptor lists for query and train images\n",
        "    query_kps_orb = []\n",
        "    query_desc_orb = []\n",
        "    train_kps_orb = []\n",
        "    train_desc_orb = []\n",
        "\n",
        "    # Generate keypoints and descriptors for query images\n",
        "    for query_image in query_images:\n",
        "      img = cv2.cvtColor(query_image, cv2.COLOR_BGR2GRAY)\n",
        "      kps, desc = orb.detectAndCompute(img, None)\n",
        "      query_kps_orb.append(kps)\n",
        "      query_desc_orb.append(desc)\n",
        "\n",
        "    # Generate keypoints and descriptors for train images\n",
        "    for train_image in train_images:\n",
        "      img = cv2.cvtColor(train_image, cv2.COLOR_BGR2GRAY)\n",
        "      kps, desc = orb.detectAndCompute(img, None)\n",
        "      train_kps_orb.append(kps)\n",
        "      train_desc_orb.append(desc)\n",
        "\n",
        "    return query_kps_orb, query_desc_orb, train_kps_orb, train_desc_orb"
      ],
      "id": "4b1223cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqxorAYXx468"
      },
      "outputs": [],
      "source": [
        "[query_kps_orb, query_desc_orb, train_kps_orb, train_desc_orb] = generateAllDescriptorsAndKeypoints(query_imgs, train_imgs)"
      ],
      "id": "gqxorAYXx468"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03aa9edf"
      },
      "outputs": [],
      "source": [
        "# Show some data\n",
        "print(len(query_kps_orb[0]))\n",
        "print(query_desc_orb[0].shape)\n",
        "print(query_desc_orb[0])"
      ],
      "id": "03aa9edf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3362cc8"
      },
      "source": [
        "\n",
        "Next, write a function called `build_db_lsh` to build a **standard** (*no multi-probe*) LSH index from a set of images:"
      ],
      "id": "b3362cc8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f4e0e3e"
      },
      "outputs": [],
      "source": [
        "def build_db_lsh(descs, tables = 6, hash_size = 12):\n",
        "    \"\"\"\n",
        "    build a standard (no multi-probe) LSH index from a set of images\n",
        "    Index a set of images using LSH.    \n",
        "    \n",
        "    - descs: A list containing numpy arrays of size (~1500, 32). Each numpy array\n",
        "        i corresponds to the ORB descriptors found at image i.\n",
        "    - tables: Number of hash tables to create.\n",
        "    - hash_size: Hash length in bits.\n",
        "  \n",
        "    RETURN: \n",
        "    - index: The trained LSH index.\n",
        "    \"\"\"  "
      ],
      "id": "7f4e0e3e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy0znJh8269l"
      },
      "outputs": [],
      "source": [
        "print(train_desc_orb[0])"
      ],
      "id": "jy0znJh8269l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80ccfabe"
      },
      "outputs": [],
      "source": [
        "# Simple example of DB construction\n",
        "index = build_db_lsh(train_desc_orb[0:2])\n",
        "print(len(index.getTrainDescriptors()))"
      ],
      "id": "80ccfabe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e544ed8d"
      },
      "source": [
        "**Q4**: In the following cell compute the resulting **mAP** of the system **using 6 tables and a hash size of 12**:"
      ],
      "id": "e544ed8d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a0ebcfb"
      },
      "outputs": [],
      "source": [
        "# Fill this variable with the resulting mAP\n",
        "mAP_lsh = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "id": "1a0ebcfb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9716be7"
      },
      "outputs": [],
      "source": [
        "print('mAP: %.5f' % mAP_lsh)"
      ],
      "id": "c9716be7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e599bd3"
      },
      "source": [
        "**Q5**: Are the results stable? Do you obtain always the same mAP? Why?"
      ],
      "id": "1e599bd3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71126dfa"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "71126dfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64381fca"
      },
      "source": [
        "**Q6**: Analyze the effect of changing the number of tables / hash size in terms of mAP and average response time. Some plots here can be useful to justify your answer."
      ],
      "id": "64381fca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7978b7c4"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "7978b7c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74726440"
      },
      "source": [
        "**Q7:** Despite the different descriptors used, compare the performance of the randomized k-d trees and LSH approaches from different points of view (accuracy, training times, querying times, ...). Some plots can be useful here to justify your answer."
      ],
      "id": "74726440"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cf65981"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "3cf65981"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5d89bb5"
      },
      "source": [
        "## Bag-of-Words\n",
        "In this section, you will implement the Bag-of-Words (BoW) model for image retrieval. Additionally, you will also implement the TF-IDF scoring scheme."
      ],
      "id": "d5d89bb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b174ede"
      },
      "source": [
        "### Download visual dictionaries\n",
        "To use a BoW model, first we need a visual vocabulary. The authors of the INRIA Holidays dataset provide some visual vocabularies, trained using a clustering method (e.g. $k$-means) in a different dataset (Flickr60K).\n",
        "\n",
        "First, download these vocabularies from [here](https://uibes-my.sharepoint.com/:u:/g/personal/egf350_id_uib_es/EY1G011OvfJOnwqWQQzmHmgBkXhLHBaK00wdizsUT252dw).\n",
        "\n",
        "> **Unzip this file into the root directory of the development kit, at the same level of the datasets.**\n",
        "\n",
        "A folder named `clust` is now available in your workspace, containing visual vocabularies of 100, 200, 500, 1K, 2K, 5K, 10K, 20K, 50K, 100K and 200K visual words. Again, these are binary files, and therefore we provide you with functions to load and index them:"
      ],
      "id": "9b174ede"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX5pyGbzyfbA"
      },
      "outputs": [],
      "source": [
        "%cd '../'"
      ],
      "id": "DX5pyGbzyfbA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk5-GMysUqZO"
      },
      "outputs": [],
      "source": [
        "unzip_tarfile(\"clust.tar.gz\")"
      ],
      "id": "Jk5-GMysUqZO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e26fefdc"
      },
      "outputs": [],
      "source": [
        "voc = rd.load_visual_vocab(\"clust/clust_flickr60_k200.fvecs\", ntrees=4)"
      ],
      "id": "e26fefdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97d05851"
      },
      "source": [
        "With this function, the corresponding vocabulary is read. Additionally, a FLANN index structure based on kd-trees is built and returned using the centroids. This is to allow a fast access when searching for the closest visual words in the vocabulary. More precisely, in this example, 4 trees are constructed using the vocabulary of 200 centroids. Now, given a query descriptor(s), you can use `match` or `knnMatch` methods as usual to search for the closest (approximate) visual word(s) in the vocabulary."
      ],
      "id": "97d05851"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95ccb141"
      },
      "source": [
        "### BoW and Inverted File\n",
        "Now, write a class called `BoW` to manage the indexing procedure. This class should make use, in addition to the visual vocabulary, an inverted file to compute similarity scores between images. Apart from the class constructor, write three methods: `build_db`, `search_image` and `compute_mAP`:"
      ],
      "id": "95ccb141"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "802ae103"
      },
      "outputs": [],
      "source": [
        "class BoW:\n",
        "    \"\"\"\n",
        "    Class to implement the BoW model + Inverted File.\n",
        "    \"\"\"\n",
        "  \n",
        "    def __init__(self, vocab_file):\n",
        "        \"\"\"\n",
        "        Class constructor. It loads the vocabulary and initializes other stuff\n",
        "        required for the CBIR system, such as the inverted file structure.\n",
        "        \"\"\"\n",
        "        self.vocab = rd.load_visual_vocab(vocab_file)\n",
        "        print(self.vocab)\n",
        "        self.nwords = self.vocab.getTrainDescriptors()[0].shape[0]\n",
        "        self.train_names = []\n",
        "        self.inv_list = {word_id: {} for word_id in range(self.nwords)}\n",
        "\n",
        "    def get_word_ids(self, descriptors):\n",
        "      \"\"\"\n",
        "      Assign a visual word id to each descriptor.\n",
        "      \n",
        "      - descriptors: A numpy array of shape (ndesc, 128). Each row is a descriptor.\n",
        "      \n",
        "      RETURN:\n",
        "      - word_ids: A numpy array of shape (ndesc,) containing the assigned visual word ids.\n",
        "      \"\"\"\n",
        "      # Compute distances to the vocabulary words\n",
        "      dists = cdist(self.vocab, descriptors, 'euclidean')\n",
        "      \n",
        "      # Assign the visual word id to each descriptor\n",
        "      word_ids = np.argmin(dists, axis=0)\n",
        "      \n",
        "      return word_ids\n",
        "\n",
        "    def build_db(self, img_names, img_descs):\n",
        "        \"\"\"\n",
        "        Build an index from a set of images. Essentially, for each image, you should\n",
        "        search its descriptors in the index in order to find the closest visual words\n",
        "        and fill the inverted file structure consequently.\n",
        "    \n",
        "        - img_names: An ordered list with the names of the train images\n",
        "        - img_descs: A list containing numpy arrays. Each numpy array i corresponds \n",
        "          to the descriptors found at image i\n",
        "        \"\"\"\n",
        "        self.train_names = img_names\n",
        "        for i, descriptors in enumerate(img_descs):\n",
        "            # Assign a visual word id to each descriptor\n",
        "            word_ids = self.get_word_ids(descriptors)\n",
        "            \n",
        "            # Update the inverted file structure\n",
        "            for j, word_id in enumerate(word_ids):\n",
        "                if i not in self.inv_list[word_id]:\n",
        "                    self.inv_list[word_id][i] = []\n",
        "                self.inv_list[word_id][i].append(j)\n",
        "\n",
        "    def search_image(self, descs):\n",
        "        \"\"\"\n",
        "        Search an image in the index.\n",
        "      \n",
        "        - descs: A numpy array. It is the set descriptors extracted from the query image.\n",
        "    \n",
        "        RETURN:\n",
        "        - An ordered list of similar images, e.g.: ['100101.jpg', '100202.jpg', ...]\n",
        "        \"\"\"\n",
        "        # query the LSH index to find the closest visual words\n",
        "        word_ids = self.vocab.get_word_ids(self.lsh.query(descs))\n",
        "        \n",
        "        # get the image IDs associated with each visual word\n",
        "        img_ids = set()\n",
        "        for word_id in word_ids:\n",
        "            img_ids.update(self.inv_file[word_id])\n",
        "        \n",
        "        # compute the image distances\n",
        "        img_dist = []\n",
        "        for img_id in img_ids:\n",
        "            dist = np.linalg.norm(self.vocab.quantize(descs) - self.vocab.quantize(self.img_descs[img_id]))\n",
        "            img_dist.append((self.img_paths[img_id], dist))\n",
        "        \n",
        "        # return the ordered list of similar images\n",
        "        return [img_path for img_path, dist in sorted(img_dist, key=lambda x: x[1])]\n",
        "        \n",
        "    def compute_mAP(self, query_names, query_descs):\n",
        "        \"\"\"\n",
        "        Perform a search for a list of query images against the database and evaluates\n",
        "        the performance of the system.\n",
        "        \n",
        "        - query_names: An ordered list with the names of query images\n",
        "        - query_descs: A list containing numpy arrays of size (ndesc_for_this_image, 128). \n",
        "              Each numpy array i corresponds to the descriptors found at image i.\n",
        "\n",
        "        RETURN:\n",
        "        - total_results: A dictionary containing, for each query image, an ordered list of the retrieved images.\n",
        "        - m_ap: Mean Average Precision averaged over all queries.\n",
        "        \"\"\"\n",
        "        total_results = {}\n",
        "        m_ap = 0.0\n",
        "        \n",
        "        # search for each query image\n",
        "        for i, (name, desc) in enumerate(zip(query_names, query_descs)):\n",
        "            print(\"Querying image {} ({}/{})...\".format(name, i+1, len(query_names)))\n",
        "            results = self.search_image(desc)\n",
        "            total_results[name] = results\n",
        "            \n",
        "            # compute average precision\n",
        "            relevant = [int(n.split(\"_\")[0] == name.split(\"_\")[0]) for n in results]\n",
        "            scores = np.arange(len(relevant), 0, -1)\n",
        "            ap = average_precision_score(relevant, scores)\n",
        "            m_ap += ap\n",
        "        \n",
        "        # average over all queries\n",
        "        m_ap /= len(query_names)\n",
        "        \n",
        "        return total_results, m_ap"
      ],
      "id": "802ae103"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efd007f2"
      },
      "outputs": [],
      "source": [
        "# Example of use\n",
        "index = BoW('clust/clust_flickr60_k200.fvecs')\n",
        "index.build_db(train_names[0:2], train_desc[0:2])"
      ],
      "id": "efd007f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b12b2730"
      },
      "outputs": [],
      "source": [
        "res = index.search_image(query_desc[0])\n",
        "print(res)"
      ],
      "id": "b12b2730"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1661e449"
      },
      "outputs": [],
      "source": [
        "results, mAP = index.compute_mAP(query_names, query_desc)\n",
        "print(results)\n",
        "print(mAP)"
      ],
      "id": "1661e449"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5046a9c"
      },
      "source": [
        "**Q8**: In the following cell compute the resulting mAP of the system **using the vocabularies of 200, 2K, 20K and 200K visual words**:"
      ],
      "id": "a5046a9c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8901aab"
      },
      "outputs": [],
      "source": [
        "# Fill these variables with the resulting mAP\n",
        "mAP_200  = 0.0\n",
        "mAP_2K   = 0.0\n",
        "mAP_20K  = 0.0\n",
        "mAP_200K = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "id": "c8901aab"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c1ed565"
      },
      "outputs": [],
      "source": [
        "print('mAP 200: %.5f' % mAP_200)\n",
        "print('mAP 2K: %.5f' % mAP_2K)\n",
        "print('mAP 20K: %.5f' % mAP_20K)\n",
        "print('mAP 200K: %.5f' % mAP_200K)"
      ],
      "id": "9c1ed565"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e818e7c"
      },
      "source": [
        "**Q9**: Compare the performances obtained on each case. Is a larger vocabulary size always better? Why or why not?"
      ],
      "id": "5e818e7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6015870"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "f6015870"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b582757"
      },
      "source": [
        "**Q10**: Analyze the effect of the vocabulary size in terms of mAP and average response time (train and query times). Are these times constant for each vocabulary? Some plots here can be useful to justify your answer."
      ],
      "id": "7b582757"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7695e01"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "a7695e01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b98f5416"
      },
      "source": [
        "**Q11**: Do the results obtained depend on the set of images used to generate the vocabulary? How can we improve the retrieval performance?"
      ],
      "id": "b98f5416"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7345924"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "a7345924"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28efbead"
      },
      "source": [
        "### TF-IDF\n",
        "As a final task of this assignment, let's implement the TF-IDF scoring scheme. Modify the `BoW` class you wrote before to include the TF-IDF weighting scheme:"
      ],
      "id": "28efbead"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "593cafcc"
      },
      "outputs": [],
      "source": [
        "class BoW_TFIDF:\n",
        "    \"\"\"\n",
        "    Class to implement the BoW model + Inverted File + TF-IDF Scoring scheme\n",
        "    \"\"\"\n",
        "  \n",
        "    def __init__(self, vocab_file):\n",
        "        \"\"\"\n",
        "        Class constructor. It loads the vocabulary and initializes other stuff\n",
        "        required for the CBIR system, such as the inverted file structure.\n",
        "        \"\"\"\n",
        "        self.vocab = rd.load_visual_vocab(vocab_file)\n",
        "        self.nwords = self.vocab.getTrainDescriptors()[0].shape[0]\n",
        "        self.train_names = []\n",
        "        self.inv_list = {word_id: {} for word_id in range(self.nwords)}        \n",
        "        self.tfidf = {}\n",
        "\n",
        "    def build_db(self, img_names, img_descs):\n",
        "        \"\"\"\n",
        "        Build an index from a set of images. Essentially, for each image, you should\n",
        "        search its descriptors in the index in order to find the closest visual words\n",
        "        and fill the inverted file structure consequently. Additionally, TF and IDF terms\n",
        "        should be computed here.\n",
        "    \n",
        "        - img_names: An ordered list with the names of the train images\n",
        "        - img_descs: A list containing numpy arrays. Each numpy array i corresponds \n",
        "          to the descriptors found at image i\n",
        "        \"\"\"\n",
        "        self.inverted_files = {}\n",
        "        for i, desc in enumerate(img_descs):\n",
        "            words = self.quantize(desc)\n",
        "            for w in set(words):\n",
        "                if w not in self.inverted_files:\n",
        "                    self.inverted_files[w] = []\n",
        "                self.inverted_files[w].append(i)\n",
        "        \n",
        "        # compute IDF weights\n",
        "        n_docs = len(img_descs)\n",
        "        df = np.zeros(self.vocab_size, dtype=np.float32)\n",
        "        for w, doc_ids in self.inverted_files.items():\n",
        "            df[w] = len(doc_ids)\n",
        "        idf = np.log((1 + n_docs) / (1 + df)) + 1\n",
        "        self.idf_weights = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "        self.idf_weights.fit(idf.reshape(1, -1))\n",
        "\n",
        "    def search_image(self, descs):\n",
        "        \"\"\"\n",
        "        Search an image in the index. Use the TF-IDF here when scoring the images.\n",
        "      \n",
        "        - descs: A numpy array. It is the set descriptors extracted from the query image.\n",
        "    \n",
        "        RETURN:\n",
        "        - An ordered list of similar images, e.g.: ['100101.jpg', '100202.jpg', ...]\n",
        "        \"\"\"\n",
        "        query_words = self.quantize(descs)\n",
        "        query_tf = np.bincount(query_words, minlength=self.vocab_size)\n",
        "        query_tf_idf = self.idf_weights.transform(query_tf.reshape(1, -1)).toarray()[0]\n",
        "        \n",
        "        # compute cosine similarity between query and documents\n",
        "        scores = []\n",
        "        for i, desc in enumerate(self.visual_words):\n",
        "            doc_tf_idf = self.idf_weights.transform(desc.reshape(1, -1)).toarray()[0]\n",
        "            sim = np.dot(query_tf_idf, doc_tf_idf) / np.linalg.norm(query_tf_idf) / np.linalg.norm(doc_tf_idf)\n",
        "            scores.append((i, sim))\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        \n",
        "        # return list of document names sorted by similarity\n",
        "        results = [img_names[i] for i, _ in scores]\n",
        "        return results\n",
        "        \n",
        "    def compute_mAP(self, query_names, query_descs):\n",
        "        \"\"\"\n",
        "        Perform a search for a list of query images against the database and evaluates\n",
        "        the performance of the system.\n",
        "        \n",
        "        - query_names: An ordered list with the names of query images\n",
        "        - query_descs: A list containing numpy arrays of size (ndesc_for_this_image, 128). \n",
        "              Each numpy array i corresponds to the descriptors found at image i.\n",
        "\n",
        "        RETURN:\n",
        "        - total_results: A dictionary containing, for each query image, an ordered list of the retrieved images.\n",
        "        - m_ap: Mean Average Precision averaged over all queries.\n",
        "        \"\"\"\n",
        "        total_results = {}\n",
        "        m_ap = 0.0\n",
        "        \n",
        "        # search for each query image\n",
        "        for i, (name, desc) in enumerate(zip(query_names, query_descs)):\n",
        "            print(\"Querying image {} ({}/{})...\".format(name, i+1, len(query_names)))\n",
        "            results = self.search_image(desc)\n",
        "            total_results[name] = results\n",
        "            \n",
        "            # compute average precision\n",
        "            relevant = [int(n.split(\"_\")[0] == name.split(\"_\")[0]) for n in results]\n",
        "            scores = np.arange(len(relevant), 0, -1)\n",
        "            ap = average_precision_score(relevant, scores)\n",
        "            m_ap += ap\n",
        "        \n",
        "        # average over all queries\n",
        "        m_ap /= len(query_names)\n",
        "        \n",
        "        return total_results, m_ap"
      ],
      "id": "593cafcc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b50bb6a4"
      },
      "outputs": [],
      "source": [
        "# Example of use\n",
        "index = BoW_TFIDF('clust/clust_flickr60_k200.fvecs')\n",
        "index.build_db(train_names[0:2], train_desc[0:2])"
      ],
      "id": "b50bb6a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6de5068e"
      },
      "outputs": [],
      "source": [
        "res = index.search_image(query_desc[0])\n",
        "print(res)"
      ],
      "id": "6de5068e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a554f0a8"
      },
      "source": [
        "**Q12**: In the following cell compute the resulting mAP of the system **using the vocabularies of 200, 2K, 20K and 200K visual words**:"
      ],
      "id": "a554f0a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6307706a"
      },
      "outputs": [],
      "source": [
        "# Fill these variables with the resulting mAP\n",
        "mAP_200  = 0.0\n",
        "mAP_2K   = 0.0\n",
        "mAP_20K  = 0.0\n",
        "mAP_200K = 0.0\n",
        "\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()"
      ],
      "id": "6307706a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba4761f6"
      },
      "outputs": [],
      "source": [
        "print('mAP 200: %.5f' % mAP_200)\n",
        "print('mAP 2K: %.5f' % mAP_2K)\n",
        "print('mAP 20K: %.5f' % mAP_20K)\n",
        "print('mAP 200K: %.5f' % mAP_200K)"
      ],
      "id": "ba4761f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aa3cf87"
      },
      "source": [
        "**Q13:** Compare the performances obtained on each case. Is a larger vocabulary size always better? Why or why not?"
      ],
      "id": "1aa3cf87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65d6852c"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "65d6852c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "752ac143"
      },
      "source": [
        "**Q14**: Analyze the effect of the vocabulary size in terms of mAP and average response time (train and query times). Are these times constant for each vocabulary? Some plots here can be useful to justify your answer."
      ],
      "id": "752ac143"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c07db06d"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "c07db06d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efc83c87"
      },
      "source": [
        "**Q15**: Do the results obtained depend on the set of images used to generate the vocabulary? How can we improve the retrieval performance?"
      ],
      "id": "efc83c87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61429d22"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "61429d22"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6465cfb0"
      },
      "source": [
        "**Q16:** How does TF-IDF affect the performance? Better or worse? Does this make sense?"
      ],
      "id": "6465cfb0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b193d85"
      },
      "source": [
        "Write here the code required to answer the questions stated above. You can add more cells (code / markdown) at this point if you need it."
      ],
      "id": "4b193d85"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b3b83bc"
      },
      "source": [
        "## Submitting your work\n",
        "\n",
        "**Important**: Please make sure that the submitted notebooks have been run and the cell outputs are visible.\n",
        "\n",
        "**Important**: Please make also sure that you have filled the **NAME** and **DNI** variables at the beginning of the notebook, **using the indicated format**.\n",
        "\n",
        "Once you have filled out the necessary code and you are happy with your solution, **save your notebook** and execute the following cell:"
      ],
      "id": "8b3b83bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ef0d318"
      },
      "outputs": [],
      "source": [
        "zip_filename = DNI + '_A2.zip'\n",
        "zf = zipfile.ZipFile(zip_filename, mode = 'w')\n",
        "\n",
        "aname = 'submitted/' + DNI + '/A2/Image_Indexing.ipynb'\n",
        "zf.write('Image_Indexing.ipynb', arcname = aname);\n",
        "\n",
        "zf.close()"
      ],
      "id": "0ef0d318"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f06aa9d3"
      },
      "source": [
        "This will generate a zip file of your code called `DNI_A2.zip` in the same directory of the assignment. This is the file that you must upload to [Aula Digital](https://uibdigital.uib.es/) to submit your work!"
      ],
      "id": "f06aa9d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55cb8ca9"
      },
      "source": [
        "---\n",
        "\n",
        "&copy; Emilio Garcia-Fidalgo, University of the Balearic Islands"
      ],
      "id": "55cb8ca9"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}